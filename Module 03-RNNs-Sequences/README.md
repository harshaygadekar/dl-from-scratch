# Module 03: Sequence Models

> Implement recurrent models and attention bridges before transformers.

---

## Overview

This module introduces temporal modeling from first principles:
- Vanilla recurrence and BPTT
- LSTM/GRU gating behavior
- Sequence masking and variable-length handling
- Encoder-decoder sequence models
- Bahdanau attention as the transformer bridge

---

## Topics

| Topic | Name | Description | Duration |
|-------|------|-------------|----------|
| 18 | Vanilla RNN | Forward/backward through time | 2-3 hrs |
| 19 | LSTM Gates | Cell-state gated recurrence | 2-3 hrs |
| 20 | GRU Variant | Compact gated recurrent design | 2-3 hrs |
| 21 | Bidirectional RNNs & Masking | Variable-length sequence handling | 2-3 hrs |
| 22 | Word Embeddings (Word2Vec Logic) | Skip-gram and negative sampling | 2-3 hrs |
| 23 | Seq2Seq Encoder-Decoder | Conditional generation loop | 2-3 hrs |
| 24 | Attention Mechanism (Bahdanau) | Alignment scores and context vectors | 3-4 hrs |

---

## Milestone

Topic 24 should produce a working attention-based seq2seq baseline that is ready to transition into transformer attention patterns.
