# Module 01: Neural Network Core

> Build foundational neural network components from scratch.

---

## ğŸ“‹ Overview

This module covers the essential building blocks of neural networks:
- Linear layers and activation functions
- Loss functions
- Complete forward and backward passes
- A working MLP from scratch

---

## ğŸ“š Topics

| Topic | Name | Description | Duration |
|-------|------|-------------|----------|
| 04 | Single Layer Perceptron | Sigmoid, BCE loss, gradient descent | 2-3 hrs |
| 05 | MLP Forward Pass | Multi-layer networks, weight initialization | 2-3 hrs |
| 06 | Backpropagation | Chain rule, backward pass, gradients | 3-4 hrs |
| 07 | Activation Functions | ReLU, Sigmoid, Tanh, Softmax + gradients | 2-3 hrs |
| 08 | Loss Functions | MSE, Cross-Entropy, Binary CE | 2-3 hrs |
| 09 | Regularization | L2, Dropout, Batch Normalization | 2-3 hrs |
| 10 | End-to-End MNIST | Complete MLP, 95% accuracy target | 3-4 hrs |

---

## ğŸ¯ Learning Objectives

After completing this module, you will:
1. Understand how linear transformations work mathematically
2. Implement various activation functions with correct gradients
3. Build loss functions for regression and classification
4. Assemble a complete MLP that can learn
5. Apply regularization techniques to prevent overfitting
6. Train a network to 95%+ accuracy on MNIST

---

## ğŸ”§ Prerequisites

- âœ… Module 00: Foundations (Tensor ops, Autograd, Optimizers)
- âœ… Understanding of matrix multiplication
- âœ… Basic calculus (chain rule)

---

## ğŸ“ˆ Difficulty Progression

```
Topic 04 (Perceptron)   â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘ Medium
Topic 05 (MLP)          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘ Medium
Topic 06 (Backprop)     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ Medium-Hard
Topic 07 (Activations)  â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘ Medium
Topic 08 (Losses)       â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘ Medium
Topic 09 (Reg/BN)       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘ Medium
Topic 10 (MNIST)        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ Medium-Hard
```

---

## â±ï¸ Estimated Time

**Total**: 18-24 hours

---

## ğŸ—‚ï¸ Directory Structure

```
Module 01-Neural-Network-Core/
â”œâ”€â”€ README.md
â”œâ”€â”€ Topic 04-Single-Layer-Perceptron/
â”œâ”€â”€ Topic 05-MLP-Forward-Pass/
â”œâ”€â”€ Topic 06-Backpropagation/
â”œâ”€â”€ Topic 07-Activation-Functions/
â”œâ”€â”€ Topic 08-Loss-Functions/
â”œâ”€â”€ Topic 09-Regularization/
â””â”€â”€ Topic 10-End-to-End-MNIST/
```

---

## ğŸ† Module Milestone

By the end of this module, you should be able to:

```python
# Train a neural network on MNIST-like data
mlp = MLP(784, [256, 128, 10])
optimizer = Adam(mlp.parameters(), lr=0.001)

for epoch in range(10):
    for x_batch, y_batch in dataloader:
        logits = mlp(x_batch)
        loss = cross_entropy(logits, y_batch)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    print(f"Epoch {epoch}: Loss = {loss.data:.4f}")
```

---

*"The neural network is simple in concept but deep in possibilities."*
