# Topic 06: Backpropagation

> **Goal**: Master the chain rule and implement backward pass.
> **Time**: 3-4 hours | **Difficulty**: Hard

---

## ğŸ¯ Learning Objectives

By the end of this topic, you will:
1. Derive gradients using the chain rule
2. Implement backward pass for all layer types
3. Understand gradient flow through networks
4. Debug gradient issues (vanishing/exploding)

---

## ğŸ“‹ The Problem

Compute gradients of the loss with respect to all parameters.

### The Chain Rule

For a composition f(g(x)):
```
âˆ‚f/âˆ‚x = âˆ‚f/âˆ‚g Ã— âˆ‚g/âˆ‚x
```

### Neural Network Backward Pass

```
Forward:  x â†’ hâ‚ â†’ hâ‚‚ â†’ y â†’ L (loss)
Backward: âˆ‚L/âˆ‚x â† âˆ‚L/âˆ‚hâ‚ â† âˆ‚L/âˆ‚hâ‚‚ â† âˆ‚L/âˆ‚y â† 1
```

### Required Implementation

```python
class Linear:
    def forward(self, x):
        self.input = x  # Cache for backward
        return x @ self.W + self.b
    
    def backward(self, grad_output):
        # grad_output = âˆ‚L/âˆ‚output
        self.grad_W = self.input.T @ grad_output
        self.grad_b = grad_output.sum(axis=0)
        grad_input = grad_output @ self.W.T
        return grad_input

class ReLU:
    def forward(self, x):
        self.mask = x > 0
        return np.maximum(0, x)
    
    def backward(self, grad_output):
        return grad_output * self.mask
```

---

## ğŸ§  Key Equations

### Linear Layer
```
Forward:  y = Wx + b
Backward: âˆ‚L/âˆ‚W = x^T Â· âˆ‚L/âˆ‚y
          âˆ‚L/âˆ‚b = sum(âˆ‚L/âˆ‚y)
          âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚y Â· W^T
```

### ReLU
```
Forward:  y = max(0, x)
Backward: âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚y Â· 1_{x>0}
```

### Softmax + Cross-Entropy
```
Forward:  p = softmax(z), L = -Î£y_true log(p)
Backward: âˆ‚L/âˆ‚z = p - y_true
```

---

## ğŸ“ File Structure

```
Topic 06-Backpropagation/
â”œâ”€â”€ README.md
â”œâ”€â”€ questions.md
â”œâ”€â”€ intuition.md
â”œâ”€â”€ math-refresh.md
â”œâ”€â”€ hints/
â”‚   â”œâ”€â”€ hint-1-chain-rule.md
â”‚   â”œâ”€â”€ hint-2-linear-backward.md
â”‚   â””â”€â”€ hint-3-full-network.md
â”œâ”€â”€ solutions/
â”‚   â”œâ”€â”€ level01_naive.py
â”‚   â”œâ”€â”€ level02_vectorized.py
â”‚   â”œâ”€â”€ level03_memory_efficient.py
â”‚   â””â”€â”€ level04_pytorch_reference.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_basic.py
â”‚   â”œâ”€â”€ test_edge.py
â”‚   â””â”€â”€ test_stress.py
â””â”€â”€ visualization.py
```

---

## ğŸ® How to Use

### Train a Network
```python
mlp = MLP([784, 256, 10])
for epoch in range(epochs):
    # Forward
    output = mlp.forward(X)
    loss = cross_entropy(output, y)
    
    # Backward
    grad = mlp.backward(y)
    
    # Update
    mlp.update(lr=0.01)
```

---

## ğŸ† Success Criteria

| Level | Requirement |
|-------|-------------|
| Level 1 | Single layer backward works |
| Level 2 | Multi-layer backward works |
| Level 3 | Gradients match numerical check |
| Level 4 | Matches PyTorch autograd |

---

## ğŸ”— Related Topics

- **Topic 02**: Autograd Engine (computational graphs)
- **Topic 05**: MLP Forward Pass (forward direction)
- **Topic 03**: Optimizers (use gradients for updates)

---

*"Backpropagation is just the chain rule applied efficiently."*
