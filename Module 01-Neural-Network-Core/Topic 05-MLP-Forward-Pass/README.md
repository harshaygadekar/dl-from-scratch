# Topic 05: MLP Forward Pass

> **Goal**: Build a multi-layer perceptron with proper initialization.
> **Time**: 2-3 hours | **Difficulty**: Medium

---

## üéØ Learning Objectives

By the end of this topic, you will:
1. Understand how layers stack to form deep networks
2. Implement Xavier and Kaiming initialization
3. Build forward pass through multiple layers
4. Understand activation function choices

---

## üìã The Problem

Implement a Multi-Layer Perceptron (MLP) that can learn non-linear functions.

### Mathematical Model

```
Layer 1: h‚ÇÅ = œÉ(W‚ÇÅ¬∑x + b‚ÇÅ)
Layer 2: h‚ÇÇ = œÉ(W‚ÇÇ¬∑h‚ÇÅ + b‚ÇÇ)
...
Output:  y = W_L¬∑h_{L-1} + b_L
```

### Required Implementation

```python
class MLP:
    def __init__(self, layer_sizes, activation='relu'):
        self.layers = []
        for i in range(len(layer_sizes) - 1):
            self.layers.append(Linear(layer_sizes[i], layer_sizes[i+1]))
    
    def forward(self, x):
        for layer in self.layers[:-1]:
            x = activation(layer(x))
        return self.layers[-1](x)  # No activation on output

class Linear:
    def __init__(self, in_features, out_features):
        self.W = initialize_weights(in_features, out_features)
        self.b = np.zeros(out_features)
```

---

## üß† Key Concepts

### 1. The Multi-Layer Architecture
```
Input    Hidden 1   Hidden 2   Output
 (3)       (4)        (4)       (2)

  x‚ÇÅ ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚ñ∫ h‚ÇÅ‚ÇÅ ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚ñ∫ h‚ÇÇ‚ÇÅ ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚ñ∫ y‚ÇÅ
       ‚îÇ         ‚îÇ         ‚îÇ
  x‚ÇÇ ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚ñ∫ h‚ÇÅ‚ÇÇ ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚ñ∫ h‚ÇÇ‚ÇÇ ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚ñ∫ y‚ÇÇ
       ‚îÇ         ‚îÇ         ‚îÇ
  x‚ÇÉ ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚ñ∫ h‚ÇÅ‚ÇÉ ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚ñ∫ h‚ÇÇ‚ÇÉ ‚îÄ‚îÄ‚îò
           ‚Üì          ‚Üì
          h‚ÇÅ‚ÇÑ        h‚ÇÇ‚ÇÑ
```

### 2. Weight Initialization Strategies
- **Xavier/Glorot**: Var(W) = 2/(n_in + n_out)
- **He/Kaiming**: Var(W) = 2/n_in (for ReLU)
- **LeCun**: Var(W) = 1/n_in

### 3. Activation Functions
- **ReLU**: max(0, x)
- **Sigmoid**: 1/(1 + e^(-x))
- **Tanh**: (e^x - e^(-x))/(e^x + e^(-x))

---

## üìÅ File Structure

```
Topic 05-MLP-Forward-Pass/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ questions.md
‚îú‚îÄ‚îÄ intuition.md
‚îú‚îÄ‚îÄ math-refresh.md
‚îú‚îÄ‚îÄ hints/
‚îÇ   ‚îú‚îÄ‚îÄ hint-1-linear-layer.md
‚îÇ   ‚îú‚îÄ‚îÄ hint-2-initialization.md
‚îÇ   ‚îî‚îÄ‚îÄ hint-3-activations.md
‚îú‚îÄ‚îÄ solutions/
‚îÇ   ‚îú‚îÄ‚îÄ level01-naive.py
‚îÇ   ‚îú‚îÄ‚îÄ level02-vectorized.py
‚îÇ   ‚îú‚îÄ‚îÄ level03-memory-efficient.py
‚îÇ   ‚îî‚îÄ‚îÄ level04-pytorch-reference.py
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_basic.py
‚îÇ   ‚îú‚îÄ‚îÄ test_edge.py
‚îÇ   ‚îî‚îÄ‚îÄ test_stress.py
‚îî‚îÄ‚îÄ visualization.py
```

---

## üéÆ How to Use

### Step 1: Create an MLP
```python
# Network: 784 -> 256 -> 128 -> 10
mlp = MLP([784, 256, 128, 10], activation='relu')
```

### Step 2: Forward Pass
```python
# Single sample
x = np.random.randn(784)
output = mlp.forward(x)  # Shape: (10,)

# Batch
X = np.random.randn(32, 784)
outputs = mlp.forward(X)  # Shape: (32, 10)
```

---

## üèÜ Success Criteria

| Level | Requirement |
|-------|-------------|
| Level 1 | Single layer forward pass works |
| Level 2 | Multi-layer forward pass works |
| Level 3 | Xavier and Kaiming initialization |
| Level 4 | Matches PyTorch nn.Linear output |

---

## üîó Related Topics

- **Topic 04**: Single Layer Perceptron (foundation)
- **Topic 06**: Backpropagation (gradients for training)
- **Topic 07**: Activation Functions (deeper dive)

---

*"Depth enables learning hierarchical representations."*
