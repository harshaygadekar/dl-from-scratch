# Topic 08: Loss Functions

> **Goal**: Implement all common loss functions with gradients.
> **Time**: 2-3 hours | **Difficulty**: Medium

---

## ğŸ¯ Learning Objectives

By the end of this topic, you will:
1. Implement MSE, MAE for regression
2. Implement Cross-Entropy for classification
3. Implement Binary Cross-Entropy
4. Understand when to use each loss

---

## ğŸ“‹ Loss Functions Overview

| Name | Formula | Use Case |
|------|---------|----------|
| MSE | (y - Å·)Â² | Regression |
| MAE | \|y - Å·\| | Robust regression |
| Cross-Entropy | -Î£ y log(p) | Multi-class |
| Binary CE | -y log(p) - (1-y)log(1-p) | Binary classification |
| Hinge | max(0, 1 - yÂ·Å·) | SVMs, margin-based |

---

## ğŸ“ File Structure

```
Topic 08-Loss-Functions/
â”œâ”€â”€ README.md
â”œâ”€â”€ questions.md
â”œâ”€â”€ intuition.md
â”œâ”€â”€ math-refresh.md
â”œâ”€â”€ hints/
â”‚   â”œâ”€â”€ hint-1-mse-mae.md
â”‚   â”œâ”€â”€ hint-2-cross-entropy.md
â”‚   â””â”€â”€ hint-3-binary-ce.md
â”œâ”€â”€ solutions/
â”‚   â”œâ”€â”€ level01_naive.py
â”‚   â”œâ”€â”€ level02_vectorized.py
â”‚   â”œâ”€â”€ level03_memory_efficient.py
â”‚   â””â”€â”€ level04_pytorch_reference.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_basic.py
â”‚   â”œâ”€â”€ test_edge.py
â”‚   â””â”€â”€ test_stress.py
â””â”€â”€ visualization.py
```

---

## ğŸ® Usage

```python
from losses import MSELoss, CrossEntropyLoss, BCELoss

# Regression
mse = MSELoss()
loss = mse.forward(predictions, targets)
grad = mse.backward()

# Multi-class classification
ce = CrossEntropyLoss()
loss = ce.forward(logits, one_hot_labels)
grad = ce.backward()

# Binary classification
bce = BCELoss()
loss = bce.forward(sigmoid_output, binary_labels)
grad = bce.backward()
```

---

## ğŸ† Success Criteria

| Level | Requirement |
|-------|-------------|
| Level 1 | All forward passes work |
| Level 2 | All backward passes work |
| Level 3 | Numerically stable |
| Level 4 | Matches PyTorch losses |

---

## ğŸ”— Related Topics

- **Topic 06**: Backpropagation (uses loss gradients)
- **Topic 07**: Activation Functions (softmax + CE combined)

---

*"The loss function is your network's compassâ€”it guides learning."*
