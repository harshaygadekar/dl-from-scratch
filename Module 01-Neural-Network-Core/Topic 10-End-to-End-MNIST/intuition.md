# Topic 10: Intuition Guide

The full pictureâ€”putting it all together.

---

## ðŸ§  The Training Loop

```
Data â†’ Forward â†’ Loss â†’ Backward â†’ Update â†’ Repeat
```

This is the heartbeat of deep learning.

---

## MNIST: The Hello World of ML

- 28Ã—28 grayscale images of digits 0-9
- 60,000 training, 10,000 test
- Simple enough to train on CPU
- Complex enough to be meaningful

---

## The Architecture

```
Input (784) â†’ Dense (256) â†’ ReLU â†’ Dropout
           â†’ Dense (128) â†’ ReLU â†’ Dropout
           â†’ Dense (10) â†’ Softmax â†’ Prediction
```

Why this architecture?
- 784 = 28 Ã— 28 (flattened image)
- Two hidden layers capture patterns
- 10 outputs = 10 digit classes

---

## Training Dynamics

**Epoch 1**: Random guessing (~10% accuracy)
**Epoch 5**: Learning patterns (~85% accuracy)
**Epoch 10**: Refined (~95% accuracy)

Watch the loss decrease and accuracy increase!

---

## Common Pitfalls

1. **Forgetting to normalize**: Pixel values 0-255 â†’ 0-1
2. **Wrong mode**: Train vs eval for dropout/batchnorm
3. **Not shuffling**: Same order each epoch = patterns
4. **Too high LR**: Loss explodes
5. **Too low LR**: Takes forever

---

*"When you can train MNIST from scratch, you understand neural networks."*
