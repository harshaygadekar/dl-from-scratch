# Module 03: RNNs & Sequences

> Process sequential data with recurrent architectures.

---

## ğŸ“‹ Overview

This module covers recurrent neural networks and sequence modeling:
- RNN cells and backpropagation through time
- LSTM and GRU architectures
- Sequence-to-sequence models
- Embeddings

---

## ğŸ“š Topics

| Topic | Name | Description | Duration |
|-------|------|-------------|----------|
| 14 | RNN Cell | Basic recurrence and BPTT | 3-4 hrs |
| 15 | LSTM | Long short-term memory | 3-4 hrs |
| 16 | GRU | Gated recurrent unit | 2-3 hrs |
| 17 | Embeddings | Word â†’ vector representations | 2-3 hrs |
| 18 | Seq2Seq | Encoder-decoder architecture | 3-4 hrs |
| 19 | Attention Basics | Pre-transformer attention | 3-4 hrs |

---

## ğŸ¯ Learning Objectives

After completing this module, you will:
1. Understand how RNNs maintain hidden state
2. Implement LSTM gates and cell states
3. Build sequence-to-sequence models
4. Implement basic attention mechanisms

---

## ğŸ”§ Prerequisites

- âœ… Module 01: Neural Network Core
- âœ… Understanding of sequences and time series
- âœ… Matrix operations

---

## ğŸ“ˆ Difficulty Progression

```
Topic 14 (RNN)      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ Medium-Hard
Topic 15 (LSTM)     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ Hard
Topic 16 (GRU)      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ Medium-Hard
Topic 17 (Embed)    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘ Medium
Topic 18 (Seq2Seq)  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘ Hard
Topic 19 (Attn)     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ Hard
```

---

## â±ï¸ Estimated Time

**Total**: 17-22 hours

---

## ğŸ—‚ï¸ Directory Structure

```
Module 03-RNNs-Sequences/
â”œâ”€â”€ README.md           # This file
â”œâ”€â”€ Topic 14-RNN-Cell/
â”œâ”€â”€ Topic 15-LSTM/
â”œâ”€â”€ Topic 16-GRU/
â”œâ”€â”€ Topic 17-Embeddings/
â”œâ”€â”€ Topic 18-Seq2Seq/
â””â”€â”€ Topic 19-Attention-Basics/
```

---

## ğŸ† Module Milestone

By the end of this module, you should be able to:

```python
# Build a character-level language model
vocab_size = 50
embed_dim = 64
hidden_dim = 128

embedding = Embedding(vocab_size, embed_dim)
lstm = LSTM(embed_dim, hidden_dim)
output = Linear(hidden_dim, vocab_size)

def forward(chars):
    x = embedding(chars)           # [seq, batch, embed]
    hidden = lstm.init_hidden()
    outputs = []
    for t in range(len(chars)):
        h, hidden = lstm(x[t], hidden)
        outputs.append(output(h))
    return outputs

# Generate text character by character!
```

---

## ğŸ” Key Interview Topics

- Why do RNNs suffer from vanishing gradients?
- How do LSTM gates solve this?
- What makes GRU simpler than LSTM?
- How does attention help with long sequences?

---

*"Sequences are the language of time."*
