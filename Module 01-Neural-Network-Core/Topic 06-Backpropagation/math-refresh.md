# Topic 06: Math Refresh

The mathematics behind backpropagation.

---

## The Chain Rule

### Single Variable
```
If y = f(g(x)), then:
dy/dx = dy/dg Ã— dg/dx
```

### Multivariable
```
If z = f(x, y) and x = g(t), y = h(t), then:
dz/dt = âˆ‚z/âˆ‚x Ã— dx/dt + âˆ‚z/âˆ‚y Ã— dy/dt
```

---

## Linear Layer Gradients

### Forward
```
y = Wx + b

Where:
- x âˆˆ â„â¿ (input)
- W âˆˆ â„áµË£â¿ (weights)
- b âˆˆ â„áµ (bias)
- y âˆˆ â„áµ (output)
```

### Backward
Given âˆ‚L/âˆ‚y (gradient from next layer):

```
âˆ‚L/âˆ‚W = xáµ€ Â· âˆ‚L/âˆ‚y

âˆ‚L/âˆ‚b = âˆ‚L/âˆ‚y  (summed over batch)

âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚y Â· Wáµ€
```

### Batch Version
For X âˆˆ â„á´®Ë£â¿ (batch of B samples):

```
âˆ‚L/âˆ‚W = Xáµ€ Â· âˆ‚L/âˆ‚Y    [shape: (n, m)]
âˆ‚L/âˆ‚b = sum(âˆ‚L/âˆ‚Y, axis=0)  [shape: (m,)]
âˆ‚L/âˆ‚X = âˆ‚L/âˆ‚Y Â· Wáµ€    [shape: (B, n)]
```

---

## Activation Gradients

### ReLU
```
Forward:  y = max(0, x)
Backward: âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚y Ã— ðŸ™{x > 0}
```

### Sigmoid
```
Forward:  y = Ïƒ(x) = 1/(1 + eâ»Ë£)
Backward: âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚y Ã— Ïƒ(x)(1 - Ïƒ(x))
                = âˆ‚L/âˆ‚y Ã— y(1 - y)
```

### Tanh
```
Forward:  y = tanh(x)
Backward: âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚y Ã— (1 - tanhÂ²(x))
                = âˆ‚L/âˆ‚y Ã— (1 - yÂ²)
```

---

## Softmax + Cross-Entropy

### Softmax
```
p_i = exp(z_i) / Î£â±¼ exp(z_j)
```

### Cross-Entropy Loss
```
L = -Î£áµ¢ yáµ¢ log(páµ¢)
```

### Combined Gradient (elegant!)
```
âˆ‚L/âˆ‚z = p - y
```

### Derivation
```
âˆ‚L/âˆ‚z_i = Î£â±¼ (âˆ‚L/âˆ‚p_j)(âˆ‚p_j/âˆ‚z_i)

âˆ‚L/âˆ‚p_j = -y_j/p_j

âˆ‚p_j/âˆ‚z_i = p_j(Î´_ij - p_i)  [Jacobian of softmax]

After algebra: âˆ‚L/âˆ‚z_i = p_i - y_i
```

---

## MSE Loss

### Forward
```
L = (1/n) Î£áµ¢ (yáµ¢ - Å·áµ¢)Â²
```

### Backward
```
âˆ‚L/âˆ‚Å· = (2/n)(Å· - y)
```

---

## Full Network Backward Pass

For network: x â†’ Lâ‚ â†’ ReLU â†’ Lâ‚‚ â†’ Softmax â†’ Loss

```
1. Forward pass, cache activations:
   zâ‚ = Wâ‚x + bâ‚
   aâ‚ = ReLU(zâ‚)
   zâ‚‚ = Wâ‚‚aâ‚ + bâ‚‚
   p = softmax(zâ‚‚)
   L = CrossEntropy(p, y)

2. Backward pass:
   âˆ‚L/âˆ‚zâ‚‚ = p - y                    [softmax+CE gradient]
   âˆ‚L/âˆ‚Wâ‚‚ = aâ‚áµ€ Â· âˆ‚L/âˆ‚zâ‚‚
   âˆ‚L/âˆ‚bâ‚‚ = sum(âˆ‚L/âˆ‚zâ‚‚)
   âˆ‚L/âˆ‚aâ‚ = âˆ‚L/âˆ‚zâ‚‚ Â· Wâ‚‚áµ€
   âˆ‚L/âˆ‚zâ‚ = âˆ‚L/âˆ‚aâ‚ Ã— ðŸ™{zâ‚ > 0}       [ReLU gradient]
   âˆ‚L/âˆ‚Wâ‚ = xáµ€ Â· âˆ‚L/âˆ‚zâ‚
   âˆ‚L/âˆ‚bâ‚ = sum(âˆ‚L/âˆ‚zâ‚)
```

---

## Numerical Gradient Check

```
âˆ‚f/âˆ‚x â‰ˆ (f(x + Îµ) - f(x - Îµ)) / (2Îµ)
```

Use Îµ â‰ˆ 1e-5 for best accuracy.

---

## Quick Reference

| Layer | Forward | Backward (âˆ‚L/âˆ‚input) |
|-------|---------|---------------------|
| Linear | y = Wx + b | âˆ‚L/âˆ‚y Â· Wáµ€ |
| ReLU | max(0, x) | âˆ‚L/âˆ‚y Ã— (x > 0) |
| Sigmoid | Ïƒ(x) | âˆ‚L/âˆ‚y Ã— y(1-y) |
| Tanh | tanh(x) | âˆ‚L/âˆ‚y Ã— (1-yÂ²) |
| Softmax+CE | p, L | p - y |
| MSE | (y-Å·)Â² | 2(Å·-y)/n |

---

*"Backpropagation is calculus on steroids."*
