# Topic 06: Intuition Guide

Understanding how gradients flow backward through a neural network.

---

## ðŸ§  The Mental Model

> **Backpropagation is blame assignment.** Each layer asks: "How much did I contribute to the error?"

---

## Mental Model 1: The Bucket Brigade ðŸª£

Imagine a line of people passing water buckets:

```
[Source] â†’ Person A â†’ Person B â†’ Person C â†’ [Fire]
              â†“           â†“           â†“
           Forward pass: passing water to fire

[Source] â† Person A â† Person B â† Person C â† [Fire]
              â†“           â†“           â†“
           Backward pass: feedback on how well each did
```

In backprop:
- **Forward**: Data flows through layers to produce output
- **Backward**: Error signal flows back, each layer learns its contribution

---

## Mental Model 2: The Chain of Responsibility â›“ï¸

Each layer in a network is like a link in a chain:

```
Input â†’ [Linear] â†’ [ReLU] â†’ [Linear] â†’ [Softmax] â†’ Loss
           â†“          â†“         â†“          â†“
          L1         A1        L2         S        (forward)

Input â† [Linear] â† [ReLU] â† [Linear] â† [Softmax] â† Loss
          dL1       dA1       dL2        dS        (backward)
```

**Chain rule in action**:
```
âˆ‚Loss/âˆ‚L1 = âˆ‚Loss/âˆ‚S Ã— âˆ‚S/âˆ‚L2 Ã— âˆ‚L2/âˆ‚A1 Ã— âˆ‚A1/âˆ‚L1
```

---

## The Key Insight: Local Gradients

Each layer only needs to know:
1. **Its local gradient**: How does its output change w.r.t. its input?
2. **Upstream gradient**: How does the loss change w.r.t. its output?

**Multiply them together** = gradient for this layer's input

```
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   x â”€â”€â†’ â”‚    Layer    â”‚ â”€â”€â†’ y
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 âˆ‚L/âˆ‚x â†â”€â”‚  âˆ‚y/âˆ‚x Ã—   â”‚ â†â”€â”€ âˆ‚L/âˆ‚y
         â”‚  âˆ‚L/âˆ‚y      â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Visualizing Gradient Flow

### Linear Layer: y = Wx + b

```
Forward:
x â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Wâ‚â‚ â”€â”€â”€â”€â”
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Wâ‚â‚‚ â”€â”€â”€â”€â”¼â”€â”€â†’ yâ‚
       â”‚                   â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Wâ‚‚â‚ â”€â”€â”€â”€â”¤
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Wâ‚‚â‚‚ â”€â”€â”€â”€â”¼â”€â”€â†’ yâ‚‚
                           
Backward (gradients flow back):
âˆ‚L/âˆ‚x â†â”€â”€â”¬â”€â”€â”€â”€â”€â”€ Wâ‚â‚áµ€ â†â”€â”€â”€â”€â”
         â”œâ”€â”€â”€â”€â”€â”€ Wâ‚â‚‚áµ€ â†â”€â”€â”€â”€â”¼â”€â”€ âˆ‚L/âˆ‚yâ‚
         â”‚                 â”‚
         â”œâ”€â”€â”€â”€â”€â”€ Wâ‚‚â‚áµ€ â†â”€â”€â”€â”€â”¤
         â””â”€â”€â”€â”€â”€â”€ Wâ‚‚â‚‚áµ€ â†â”€â”€â”€â”€â”¼â”€â”€ âˆ‚L/âˆ‚yâ‚‚
```

### ReLU: y = max(0, x)

```
Forward:          Backward:
x > 0:            gradient passes through
x = 3 â†’ y = 3    âˆ‚L/âˆ‚y = 1 â†’ âˆ‚L/âˆ‚x = 1

x â‰¤ 0:            gradient is blocked
x = -2 â†’ y = 0   âˆ‚L/âˆ‚y = 1 â†’ âˆ‚L/âˆ‚x = 0
```

ReLU is like a **gate**: open for positive, closed for negative.

---

## Why Caching Matters

During forward pass, we cache values needed for backward:

```python
class Linear:
    def forward(self, x):
        self.input = x  # CACHE! Needed for âˆ‚L/âˆ‚W
        return x @ self.W + self.b
    
    def backward(self, grad_output):
        # Use cached input to compute weight gradient
        self.grad_W = self.input.T @ grad_output
        ...
```

**Without caching**: Would need to recompute forward pass during backward
**With caching**: One forward, one backward (efficient)

---

## Gradient Accumulation

For batches, gradients accumulate:

```
Batch of 32 samples, each gives a gradient
Final gradient = average of all 32 gradients

âˆ‚L/âˆ‚W = (1/32) Ã— Î£áµ¢ âˆ‚Láµ¢/âˆ‚W
```

---

## Intuition Checkpoints âœ…

Before moving on, understand:

1. **What does backprop compute?**
   <details><summary>Answer</summary>The gradient of the loss with respect to all parameters (âˆ‚L/âˆ‚W, âˆ‚L/âˆ‚b for each layer).</details>

2. **Why do we need the chain rule?**
   <details><summary>Answer</summary>Because the loss depends on parameters through many intermediate computations. The chain rule lets us break this into local gradients.</details>

3. **What does each layer pass backward?**
   <details><summary>Answer</summary>The gradient of the loss with respect to its input (âˆ‚L/âˆ‚x), so the previous layer can continue the chain.</details>

4. **Why cache the forward pass inputs?**
   <details><summary>Answer</summary>We need the input x to compute âˆ‚L/âˆ‚W = xáµ€ Â· âˆ‚L/âˆ‚y. Without caching, we'd need to recompute.</details>

---

*"Forward pass is computation. Backward pass is learning."*
