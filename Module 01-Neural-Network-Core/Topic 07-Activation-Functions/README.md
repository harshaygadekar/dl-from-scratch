# Topic 07: Activation Functions

> **Goal**: Implement all common activations with correct gradients.
> **Time**: 2-3 hours | **Difficulty**: Medium

---

## ğŸ¯ Learning Objectives

By the end of this topic, you will:
1. Implement ReLU, LeakyReLU, ELU, GELU
2. Implement Sigmoid, Tanh, Softmax
3. Derive and implement gradients for each
4. Understand when to use each activation

---

## ğŸ“‹ Activations Overview

| Name | Formula | Range | Use Case |
|------|---------|-------|----------|
| ReLU | max(0, x) | [0, âˆ) | Hidden layers |
| LeakyReLU | max(Î±x, x) | (-âˆ, âˆ) | Prevent dying ReLU |
| Sigmoid | 1/(1+eâ»Ë£) | (0, 1) | Binary output |
| Tanh | (eË£-eâ»Ë£)/(eË£+eâ»Ë£) | (-1, 1) | Zero-centered |
| Softmax | eË£â±/Î£eË£Ê² | (0, 1) | Multi-class |
| GELU | xÂ·Î¦(x) | (-0.17, âˆ) | Transformers |

---

## ğŸ“ File Structure

```
Topic 07-Activation-Functions/
â”œâ”€â”€ README.md
â”œâ”€â”€ questions.md
â”œâ”€â”€ intuition.md
â”œâ”€â”€ math-refresh.md
â”œâ”€â”€ hints/
â”‚   â”œâ”€â”€ hint-1-relu-family.md
â”‚   â”œâ”€â”€ hint-2-sigmoid-tanh.md
â”‚   â””â”€â”€ hint-3-softmax.md
â”œâ”€â”€ solutions/
â”‚   â”œâ”€â”€ level01_naive.py
â”‚   â”œâ”€â”€ level02_vectorized.py
â”‚   â”œâ”€â”€ level03_memory_efficient.py
â”‚   â””â”€â”€ level04_pytorch_reference.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_basic.py
â”‚   â”œâ”€â”€ test_edge.py
â”‚   â””â”€â”€ test_stress.py
â””â”€â”€ visualization.py
```

---

## ğŸ® Usage

```python
# All activations have forward and backward
from activations import ReLU, Sigmoid, Softmax

relu = ReLU()
y = relu.forward(x)
grad_x = relu.backward(grad_y)

# Softmax + CrossEntropy combined for efficiency
loss, grad = softmax_cross_entropy(logits, labels)
```

---

## ğŸ† Success Criteria

| Level | Requirement |
|-------|-------------|
| Level 1 | All forward passes work |
| Level 2 | All backward passes work |
| Level 3 | Numerical gradient check passes |
| Level 4 | Matches PyTorch activations |

---

## ğŸ”— Related Topics

- **Topic 05**: MLP Forward Pass (uses activations)
- **Topic 06**: Backpropagation (gradient computation)
- **Topic 08**: Loss Functions (often combined with softmax)

---

*"The activation function is the 'spark' that makes neural networks non-linear."*
