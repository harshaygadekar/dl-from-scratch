# Topic 09: Regularization

> **Goal**: Implement L2 regularization, Dropout, and Batch Normalization.
> **Time**: 2-3 hours | **Difficulty**: Medium

---

## ğŸ¯ Learning Objectives

By the end of this topic, you will:
1. Implement L2 (weight decay) regularization
2. Build Dropout with proper train/eval modes
3. Implement Batch Normalization with running statistics
4. Understand when and why to use each technique

---

## ğŸ“‹ Techniques Overview

| Technique | Purpose | When to Use |
|-----------|---------|-------------|
| L2 Regularization | Prevent large weights | Always (as weight decay) |
| Dropout | Prevent co-adaptation | Hidden layers, large networks |
| Batch Normalization | Stabilize training | Deep networks, faster convergence |

---

## ğŸ“ File Structure

```
Topic 09-Regularization/
â”œâ”€â”€ README.md
â”œâ”€â”€ questions.md
â”œâ”€â”€ intuition.md
â”œâ”€â”€ math-refresh.md
â”œâ”€â”€ hints/
â”‚   â”œâ”€â”€ hint-1-l2-regularization.md
â”‚   â”œâ”€â”€ hint-2-dropout.md
â”‚   â””â”€â”€ hint-3-batch-norm.md
â”œâ”€â”€ solutions/
â”‚   â”œâ”€â”€ level01_naive.py
â”‚   â”œâ”€â”€ level02_vectorized.py
â”‚   â”œâ”€â”€ level03_memory_efficient.py
â”‚   â””â”€â”€ level04_pytorch_reference.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_basic.py
â”‚   â”œâ”€â”€ test_edge.py
â”‚   â””â”€â”€ test_stress.py
â””â”€â”€ visualization.py
```

---

## ğŸ® Usage

```python
# L2 Regularization
loss = cross_entropy(logits, y) + 0.01 * l2_loss(model.parameters())

# Dropout
dropout = Dropout(p=0.5)
dropout.train()  # Enable dropout
h = dropout(h)
dropout.eval()   # Disable dropout

# Batch Normalization
bn = BatchNorm1d(num_features=64)
bn.train()  # Use batch stats
h = bn(h)
bn.eval()   # Use running stats
```

---

## ğŸ† Success Criteria

| Level | Requirement |
|-------|-------------|
| Level 1 | L2, Dropout, BN forward pass |
| Level 2 | Correct backward passes |
| Level 3 | Train vs eval modes working |
| Level 4 | Matches PyTorch BatchNorm |

---

*"Regularization is the art of being just uncertain enough."*
